{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from utils import print_results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function reads in a textfile and fixes an issue with \"\\\\\"\n",
    "def filereader(path): \n",
    "  with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "      yield line.strip().replace(\"\\\\\",\"\")\n",
    "\n",
    "s = next(filereader('trees/train.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also need the following function, but you can ignore this for now.\n",
    "# It is explained later on.\n",
    "\n",
    "def transitions_from_treestring(s):\n",
    "  s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
    "  s = re.sub(\"\\)\", \" )\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\)\", \"1\", s)\n",
    "  return list(map(int, s.split()))\n",
    "\n",
    "def tokens_from_treestring(s):\n",
    "  \"\"\"extract the tokens from a sentiment tree\"\"\"\n",
    "  return re.sub(r\"\\([0-9] |\\)\", \"\", s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from nltk import Tree\n",
    "\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"tokens\", \"tree\", \"label\", \"transitions\"])\n",
    "   \n",
    "\n",
    "def examplereader(path, lower=False):\n",
    "  \"\"\"Returns all examples in a file one by one.\"\"\"\n",
    "  for line in filereader(path):\n",
    "    line = line.lower() if lower else line\n",
    "    tokens = tokens_from_treestring(line)\n",
    "    tree = Tree.fromstring(line)  # use NLTK's Tree\n",
    "    label = int(line[1])\n",
    "    trans = transitions_from_treestring(line)\n",
    "    yield Example(tokens=tokens, tree=tree, label=label, transitions=trans)\n",
    "\n",
    "# Let's load the data into memory.\n",
    "LOWER = False  # we will keep the original casing\n",
    "train_data = list(examplereader(\"trees/train.txt\", lower=LOWER))\n",
    "dev_data = list(examplereader(\"trees/dev.txt\", lower=LOWER))\n",
    "test_data = list(examplereader(\"trees/test.txt\", lower=LOWER))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a word to an ID (w2i)\n",
    "# and back (i2w).\n",
    "\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "  \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "  def __repr__(self):\n",
    "    return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "  def __reduce__(self):\n",
    "    return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "  \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.freqs = OrderedCounter()\n",
    "    self.w2i = {}\n",
    "    self.i2w = []\n",
    "\n",
    "  def count_token(self, t):\n",
    "    self.freqs[t] += 1\n",
    "    \n",
    "  def add_token(self, t):\n",
    "    self.w2i[t] = len(self.w2i)\n",
    "    self.i2w.append(t)    \n",
    "    \n",
    "  def build(self, min_freq=0):\n",
    "    '''\n",
    "    min_freq: minimum number of occurrences for a word to be included  \n",
    "              in the vocabulary\n",
    "    '''\n",
    "    self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
    "    self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)   \n",
    "    \n",
    "    tok_freq = list(self.freqs.items())\n",
    "    tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "    for tok, freq in tok_freq:\n",
    "      if freq >= min_freq:\n",
    "        self.add_token(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18280\n"
     ]
    }
   ],
   "source": [
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "v = Vocabulary()\n",
    "for data_set in (train_data,):\n",
    "  for ex in data_set:\n",
    "    for token in ex.tokens:\n",
    "      v.count_token(token)\n",
    "\n",
    "v.build()\n",
    "print(\"Vocabulary size:\", len(v.w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "# And let's also create the opposite mapping.\n",
    "# We won't use a Vocabulary for this (although we could), since the labels\n",
    "# are already numeric.\n",
    "t2i = OrderedDict({p : i for p, i in zip(i2t, range(len(i2t)))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This downloads the word2vec 300D Google News vectors \n",
    "# # The file has been truncated to only contain words that appear in our data set.\n",
    "# # You can find the original file here: https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "# # You only need to do this once.\n",
    "# # Please comment this out after downloading.\n",
    "# !wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18922\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 300\n",
    "v_embed = Vocabulary()\n",
    "\n",
    "vectors = [np.zeros(embedding_size), np.zeros(embedding_size)]\n",
    "with open('googlenews.word2vec.300d.txt', 'r') as fin:\n",
    "    for embedding in fin:\n",
    "        word, vector = embedding.split(None, maxsplit=1)\n",
    "        vectors.append(vector.split())\n",
    "        v_embed.count_token(word)\n",
    "\n",
    "v_embed.build()        \n",
    "print(\"Vocabulary size:\", len(v_embed.w2i))\n",
    "vectors = np.stack(vectors, axis=0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.10.0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using torch\", torch.__version__) \n",
    "\n",
    "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
    "# This cell selects the GPU if one is available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# When running on the CuDNN backend two further options must be set for reproducibility\n",
    "if torch.cuda.is_available():\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-ary Tree-LSTM vs Child-Sum Tree-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline training\n",
    "best_iters, train_accs, dev_accs, test_accs = [], [], [], []\n",
    "for seed in [1,2,3]:\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  tree_model = TreeLSTMClassifier(\n",
    "      len(v_embed.w2i), 300, 150, len(t2i), v_embed)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    tree_model.embed.weight.requires_grad = False\n",
    "    \n",
    "  model = tree_model.to(device)\n",
    "\n",
    "  optimizer = torch.optim.Adagrad(model.parameters(), lr=0.05)\n",
    "    \n",
    "  best_iter, train_acc, dev_acc, test_acc = train_model(\n",
    "        model, optimizer, train_data, dev_data, \n",
    "        test_data, num_iterations=10000, \n",
    "        print_every=250, eval_every=250,\n",
    "        prep_fn=prepare_treelstm_minibatch,\n",
    "        eval_fn=evaluate,\n",
    "        batch_fn=get_minibatch,\n",
    "        batch_size=25, eval_batch_size=25)\n",
    "\n",
    "  best_iters.append(best_iter)\n",
    "  train_accs.append(train_acc)\n",
    "  dev_accs.append(dev_acc)\n",
    "  test_accs.append(test_acc)\n",
    "\n",
    "print_results(best_iters, train_accs, dev_accs, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's train the Child-Sum Tree LSTM!\n",
    "from LSTM import ChildSumTreeLSTMClassifier\n",
    "\n",
    "best_iters, train_accs, dev_accs, test_accs = [], [], [], []\n",
    "for seed in [1, 2, 3]:\n",
    "    torch.manual_seed(seed)\n",
    "    tree_model = ChildSumTreeLSTMClassifier(\n",
    "        len(v_embed.w2i), 300, 150, len(t2i), v_embed)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "        tree_model.embed.weight.requires_grad = False\n",
    "\n",
    "    model = tree_model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=0.05)\n",
    "\n",
    "    best_iter, train_acc, dev_acc, test_acc = train_model(\n",
    "        model, optimizer, train_data, dev_data,\n",
    "        test_data, num_iterations=30000,\n",
    "        print_every=250, eval_every=250,\n",
    "        prep_fn=prepare_treelstm_minibatch,\n",
    "        eval_fn=evaluate,\n",
    "        batch_fn=get_minibatch,\n",
    "        batch_size=25, eval_batch_size=25)\n",
    "\n",
    "    best_iters.append(best_iter)\n",
    "    train_accs.append(train_acc)\n",
    "    dev_accs.append(dev_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "\n",
    "print_results(best_iters, train_accs, dev_accs, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae4a5fe2b1d82be109c3ad18de05b1258fde46c60d8d1e7736bcf82c0d402c28"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp1labs': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
